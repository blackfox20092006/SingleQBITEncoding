{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pennylane pennylane-lightning torch torchvision matplotlib\n!pip install jax==0.4.28 jaxlib==0.4.28","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings, os\nimport numpy as np\nwarnings.filterwarnings(\"ignore\")\nnp.seterr(all='ignore')\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('********************************************************************************************************************')\nprint('Single QBIT Encoding for MNIST dataset')\nprint('https://ieeexplore.ieee.org/abstract/document/9798852')\nimport pennylane as qml\nfrom pennylane import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nn_epochs = 6\nbatch_size = 1\nlearning_rate = 1e-4\nimg_size = 12\ntransform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor()\n])\ntrain_raw = datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\ntest_raw = datasets.MNIST(root=\".\", train=False, download=True, transform=transform)\ntrain_data = [(x, y) for x, y in train_raw if y in [0, 1]]\ntest_data = [(x, y) for x, y in test_raw if y in [0, 1]]\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=1)\ndev = qml.device(\"default.qubit\", wires=1)\ndef single_qubit_encode(x, theta, phi):\n    for i in range(0, len(x), 3):\n        x_pad = x[i:i+3] + [0]*(3 - len(x[i:i+3]))\n        beta  = theta[0] + x_pad[0]*phi[0]\n        gamma = theta[1] + x_pad[1]*phi[1]\n        delta = theta[2] + x_pad[2]*phi[2]\n        qml.RZ(beta, wires=0)\n        qml.RY(gamma, wires=0)\n        qml.RZ(delta, wires=0)\n@qml.qnode(dev, interface=\"torch\")\ndef quantum_circuit(inputs, theta, phi):\n    qml.Hadamard(wires=0)\n    single_qubit_encode(inputs, theta, phi)\n    return qml.expval(qml.PauliZ(wires=0))\nclass QuantumNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.theta = nn.Parameter(torch.randn(3) * 0.1)\n        self.phi   = nn.Parameter(torch.randn(3) * 0.1)\n    def forward(self, x):\n      x = x.view(-1).to(torch.float32).tolist()\n      result = quantum_circuit(x, self.theta, self.phi)\n      prob_0 = (1 + result) / 2\n      return prob_0.unsqueeze(0).to(torch.float32)\nmodel = QuantumNet()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = nn.MSELoss()\nfor epoch in range(n_epochs):\n    total_loss, correct = 0, 0\n    for img, label in train_loader:\n        target = torch.tensor([1.0], dtype=torch.float32) if label.item() == 0 else torch.tensor([0.0], dtype=torch.float32)\n        optimizer.zero_grad()\n        output = model(img)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        pred = 0 if output >= 0.5 else 1\n        if pred == label.item():\n            correct += 1\n    acc = correct / len(train_loader)\n    print(f\"Epoch {epoch+1:2d} | Loss: {total_loss:.4f} | Accuracy: {acc*100:.2f}%\")\ncorrect = 0\nwith torch.no_grad():\n    for img, label in test_loader:\n        output = model(img)\n        pred = 0 if output >= 0.5 else 1\n        if pred == label.item():\n            correct += 1\nprint(f\"\\nTest Accuracy: {correct / len(test_loader) * 100:.2f}%\")\nprint('********************************************************************************************************************')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Simple CNN')\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nn_epochs = 6\nbatch_size = 1\nlearning_rate = 1e-4\nimg_size = 12\ntransform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\ntrain_raw = datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\ntest_raw = datasets.MNIST(root=\".\", train=False, download=True, transform=transform)\ntrain_data = [(x, y) for x, y in train_raw if y in [0, 1]]\ntest_data = [(x, y) for x, y in test_raw if y in [0, 1]]\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=1)\nclass SampleCNN(nn.Module):\n    def __init__(self):\n        super(TraditionalCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.fc1_input_features = 32 * (img_size // 4) * (img_size // 4)\n        self.fc1 = nn.Linear(self.fc1_input_features, 2)\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = x.view(-1, self.fc1_input_features)\n        return self.fc1(x)\nmodel_dl = SampleCNN()\noptimizer_dl = torch.optim.Adam(model_dl.parameters(), lr=learning_rate)\nloss_fn_dl = nn.CrossEntropyLoss()\nfor epoch in range(n_epochs):\n    total_loss, correct = 0, 0\n    for img, label in train_loader:\n        target_dl = label.to(torch.long)\n        optimizer_dl.zero_grad()\n        output_dl = model_dl(img)\n        loss_dl = loss_fn_dl(output_dl, target_dl)\n        loss_dl.backward()\n        optimizer_dl.step()\n        total_loss += loss_dl.item()\n        pred_dl = output_dl.argmax(dim=1, keepdim=True)\n        correct += pred_dl.eq(target_dl.view_as(pred_dl)).sum().item()\n    acc_dl = correct / len(train_loader)\n    print(f\"Epoch {epoch+1:2d} | Loss: {total_loss:.4f} | Accuracy: {acc_dl*100:.2f}%\")\ncorrect = 0\nwith torch.no_grad():\n    for img, label in test_loader:\n        output_dl = model_dl(img)\n        pred_dl = output_dl.argmax(dim=1, keepdim=True)\n        if pred_dl.item() == label.item():\n            correct += 1\nprint(f\"\\nTest Accuracy: {correct / len(test_loader) * 100:.2f}%\")\nprint('********************************************************************************************************************')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}